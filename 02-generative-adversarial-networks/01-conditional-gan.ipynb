{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Adversarial Networks (GANs) are a type of machine learning framework that pits two neural networks against each other, enabling the generation of new data that mimics an existing dataset.\n",
    "\n",
    "Here's how GANs work:\n",
    "\n",
    "1. **Generator:** One network, the generator, creates new data instances. It starts by generating random samples and aims to produce data that is indistinguishable from the real data it's been trained on. Initially, its generated samples might be poor, but through training, it improves its ability to create realistic data.\n",
    "\n",
    "2. **Discriminator:** The other network, the discriminator, evaluates the data it receives and tries to distinguish between real and generated data. Its task is to become proficient at differentiating the real data from the fake data generated by the generator.\n",
    "\n",
    "The training process involves a back-and-forth competition between these networks:\n",
    "\n",
    "- The generator tries to produce increasingly realistic data to fool the discriminator.\n",
    "- Simultaneously, the discriminator works to become better at identifying real from fake data.\n",
    "\n",
    "As training progresses, both networks improve. The generator gets better at creating realistic data, while the discriminator becomes more skilled at telling real from generated data.\n",
    "\n",
    "The ultimate aim is for the generator to produce data that's so convincing that the discriminator can't distinguish it from the real data.\n",
    "\n",
    "Applications of GANs span various fields, including image generation, video synthesis, text-to-image synthesis, and even generating realistic-sounding speech. They've been used in creating deep fakes, image enhancement, artistic style transfer, and more, showcasing their versatility in generating complex, high-dimensional data distributions.\n",
    "\n",
    "1. **Conditional GANs (cGANs):** These GANs condition both the generator and discriminator on some additional information, such as class labels, text descriptions, or other structured data. They allow for more control over the generated output.\n",
    "\n",
    "2. **Deep Convolutional GANs (DCGANs):** DCGANs leverage convolutional neural networks in both the generator and discriminator. They are specifically designed for image generation tasks, showing improved stability and performance.\n",
    "\n",
    "3. **Wasserstein GANs (WGANs):** WGANs use the Wasserstein distance (also known as Earth Moverâ€™s Distance) as a metric for training stability, leading to more stable training dynamics and better convergence properties.\n",
    "\n",
    "4. **Progressive GANs:** These networks gradually increase the resolution of generated images during training. They start with low-resolution images and incrementally add details, resulting in high-resolution, realistic outputs.\n",
    "\n",
    "5. **CycleGANs:** CycleGANs specialize in learning mappings between two different domains without paired data. They excel in image-to-image translation tasks, such as transforming images from summer to winter or changing the style of paintings without a one-to-one correspondence in the training data.\n",
    "\n",
    "6. **StyleGANs:** StyleGANs focus on controlling specific aspects of image generation, like image styles, attributes, or features. They have been used to generate highly realistic and diverse images with controllable visual features.\n",
    "\n",
    "7. **BigGANs:** These are GANs designed for generating high-quality images at high resolutions. They incorporate techniques like large batch training and architectures to generate high-fidelity images.\n",
    "\n",
    "8. **Self-Attention GANs (SAGANs):** SAGANs introduce self-attention mechanisms into GANs to enable better modeling of long-range dependencies in images, improving the generation quality and coherence.\n",
    "\n",
    "These types of GANs have distinct architectures or training methodologies tailored for specific challenges or enhancements in generating data, whether it's images, text, or other types of content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Conditional GANs (cGANs):** These GANs condition both the generator and discriminator on some additional information, such as class labels, text descriptions, or other structured data. They allow for more control over the generated output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The code has been adapted from the excellent CGAN tutorial created by Sayak Paul, available on the Keras website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Github Projects\\generative-AI\\gai\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import (\n",
    "    layers,\n",
    "    models,\n",
    "    callbacks,\n",
    "    utils,\n",
    "    metrics,\n",
    "    optimizers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "CHANNELS = 3\n",
    "CLASSES = 2\n",
    "BATCH_SIZE = 128\n",
    "Z_DIM = 32\n",
    "LEARNING_RATE = 0.00005\n",
    "ADAM_BETA_1 = 0.5\n",
    "ADAM_BETA_2 = 0.999\n",
    "EPOCHS = 20\n",
    "CRITIC_STEPS = 3\n",
    "GP_WEIGHT = 10.0\n",
    "LOAD_MODEL = False\n",
    "ADAM_BETA_1 = 0.5\n",
    "ADAM_BETA_2 = 0.9\n",
    "LABEL = \"Blond_Hair\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['File_name', '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive',\n",
      "       'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose',\n",
      "       'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows',\n",
      "       'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair',\n",
      "       'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open',\n",
      "       'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin',\n",
      "       'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns',\n",
      "       'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings',\n",
      "       'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace',\n",
      "       'Wearing_Necktie', 'Young'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_name</th>\n",
       "      <th>5_o_Clock_Shadow</th>\n",
       "      <th>Arched_Eyebrows</th>\n",
       "      <th>Attractive</th>\n",
       "      <th>Bags_Under_Eyes</th>\n",
       "      <th>Bald</th>\n",
       "      <th>Bangs</th>\n",
       "      <th>Big_Lips</th>\n",
       "      <th>Big_Nose</th>\n",
       "      <th>Black_Hair</th>\n",
       "      <th>...</th>\n",
       "      <th>Sideburns</th>\n",
       "      <th>Smiling</th>\n",
       "      <th>Straight_Hair</th>\n",
       "      <th>Wavy_Hair</th>\n",
       "      <th>Wearing_Earrings</th>\n",
       "      <th>Wearing_Hat</th>\n",
       "      <th>Wearing_Lipstick</th>\n",
       "      <th>Wearing_Necklace</th>\n",
       "      <th>Wearing_Necktie</th>\n",
       "      <th>Young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000003.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000004.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000005.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    File_name  5_o_Clock_Shadow  Arched_Eyebrows  Attractive  Bags_Under_Eyes  \\\n",
       "0  000001.jpg                -1                1           1               -1   \n",
       "1  000002.jpg                -1               -1          -1                1   \n",
       "2  000003.jpg                -1               -1          -1               -1   \n",
       "3  000004.jpg                -1               -1           1               -1   \n",
       "4  000005.jpg                -1                1           1               -1   \n",
       "\n",
       "   Bald  Bangs  Big_Lips  Big_Nose  Black_Hair  ...  Sideburns  Smiling  \\\n",
       "0    -1     -1        -1        -1          -1  ...         -1        1   \n",
       "1    -1     -1        -1         1          -1  ...         -1        1   \n",
       "2    -1     -1         1        -1          -1  ...         -1       -1   \n",
       "3    -1     -1        -1        -1          -1  ...         -1       -1   \n",
       "4    -1     -1         1        -1          -1  ...         -1       -1   \n",
       "\n",
       "   Straight_Hair  Wavy_Hair  Wearing_Earrings  Wearing_Hat  Wearing_Lipstick  \\\n",
       "0              1         -1                 1           -1                 1   \n",
       "1             -1         -1                -1           -1                -1   \n",
       "2             -1          1                -1           -1                -1   \n",
       "3              1         -1                 1           -1                 1   \n",
       "4             -1         -1                -1           -1                 1   \n",
       "\n",
       "   Wearing_Necklace  Wearing_Necktie  Young  \n",
       "0                -1               -1      1  \n",
       "1                -1               -1      1  \n",
       "2                -1               -1      1  \n",
       "3                 1               -1      1  \n",
       "4                -1               -1      1  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the label dataset\n",
    "attributes = pd.read_csv('list_attr_celeba.csv')\n",
    "print(attributes.columns)\n",
    "attributes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "labels = attributes[LABEL].tolist()\n",
    "int_labels = [x if x == 1 else 0 for x in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 202599 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = utils.image_dataset_from_directory(\n",
    "    \"img_align_celeba\",\n",
    "    labels=int_labels,\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    interpolation=\"bilinear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "def preprocess(img):\n",
    "    \"\"\"\n",
    "    Normalize and reshape the images\n",
    "    \"\"\"\n",
    "    img = (tf.cast(img, \"float32\") - 127.5) / 127.5\n",
    "    return img\n",
    "\n",
    "\n",
    "train = train_data.map(\n",
    "    lambda x, y: (preprocess(x), tf.one_hot(y, depth=CLASSES))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(dataset):\n",
    "    batch = dataset.take(1).get_single_element()\n",
    "    if isinstance(batch, tuple):\n",
    "        batch = batch[0]\n",
    "    return batch.numpy()\n",
    "\n",
    "# Show some faces from the training set\n",
    "train_sample = sample_batch(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(train_sample, cmap=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n",
    "label_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CLASSES))\n",
    "x = layers.Concatenate(axis=-1)([critic_input, label_input])\n",
    "x = layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\")(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\")(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\")(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\")(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Conv2D(1, kernel_size=4, strides=1, padding=\"valid\")(x)\n",
    "critic_output = layers.Flatten()(x)\n",
    "\n",
    "critic = models.Model([critic_input, label_input], critic_output)\n",
    "critic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input = layers.Input(shape=(Z_DIM,))\n",
    "label_input = layers.Input(shape=(CLASSES,))\n",
    "x = layers.Concatenate(axis=-1)([generator_input, label_input])\n",
    "x = layers.Reshape((1, 1, Z_DIM + CLASSES))(x)\n",
    "x = layers.Conv2DTranspose(\n",
    "    128, kernel_size=4, strides=1, padding=\"valid\", use_bias=False\n",
    ")(x)\n",
    "x = layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = layers.Conv2DTranspose(\n",
    "    128, kernel_size=4, strides=2, padding=\"same\", use_bias=False\n",
    ")(x)\n",
    "x = layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = layers.Conv2DTranspose(\n",
    "    128, kernel_size=4, strides=2, padding=\"same\", use_bias=False\n",
    ")(x)\n",
    "x = layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = layers.Conv2DTranspose(\n",
    "    64, kernel_size=4, strides=2, padding=\"same\", use_bias=False\n",
    ")(x)\n",
    "x = layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "generator_output = layers.Conv2DTranspose(\n",
    "    CHANNELS, kernel_size=4, strides=2, padding=\"same\", activation=\"tanh\"\n",
    ")(x)\n",
    "generator = models.Model([generator_input, label_input], generator_output)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalWGAN(models.Model):\n",
    "    def __init__(self, critic, generator, latent_dim, critic_steps, gp_weight):\n",
    "        super(ConditionalWGAN, self).__init__()\n",
    "        self.critic = critic\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.critic_steps = critic_steps\n",
    "        self.gp_weight = gp_weight\n",
    "\n",
    "    def compile(self, c_optimizer, g_optimizer):\n",
    "        super(ConditionalWGAN, self).compile(run_eagerly=True)\n",
    "        self.c_optimizer = c_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.c_wass_loss_metric = metrics.Mean(name=\"c_wass_loss\")\n",
    "        self.c_gp_metric = metrics.Mean(name=\"c_gp\")\n",
    "        self.c_loss_metric = metrics.Mean(name=\"c_loss\")\n",
    "        self.g_loss_metric = metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.c_loss_metric,\n",
    "            self.c_wass_loss_metric,\n",
    "            self.c_gp_metric,\n",
    "            self.g_loss_metric,\n",
    "        ]\n",
    "\n",
    "    def gradient_penalty(\n",
    "        self, batch_size, real_images, fake_images, image_one_hot_labels\n",
    "    ):\n",
    "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        diff = fake_images - real_images\n",
    "        interpolated = real_images + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            pred = self.critic(\n",
    "                [interpolated, image_one_hot_labels], training=True\n",
    "            )\n",
    "\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, data):\n",
    "        real_images, one_hot_labels = data\n",
    "\n",
    "        image_one_hot_labels = one_hot_labels[:, None, None, :]\n",
    "        image_one_hot_labels = tf.repeat(\n",
    "            image_one_hot_labels, repeats=IMAGE_SIZE, axis=1\n",
    "        )\n",
    "        image_one_hot_labels = tf.repeat(\n",
    "            image_one_hot_labels, repeats=IMAGE_SIZE, axis=2\n",
    "        )\n",
    "\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        for i in range(self.critic_steps):\n",
    "            random_latent_vectors = tf.random.normal(\n",
    "                shape=(batch_size, self.latent_dim)\n",
    "            )\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                fake_images = self.generator(\n",
    "                    [random_latent_vectors, one_hot_labels], training=True\n",
    "                )\n",
    "\n",
    "                fake_predictions = self.critic(\n",
    "                    [fake_images, image_one_hot_labels], training=True\n",
    "                )\n",
    "                real_predictions = self.critic(\n",
    "                    [real_images, image_one_hot_labels], training=True\n",
    "                )\n",
    "\n",
    "                c_wass_loss = tf.reduce_mean(fake_predictions) - tf.reduce_mean(\n",
    "                    real_predictions\n",
    "                )\n",
    "                c_gp = self.gradient_penalty(\n",
    "                    batch_size, real_images, fake_images, image_one_hot_labels\n",
    "                )\n",
    "                c_loss = c_wass_loss + c_gp * self.gp_weight\n",
    "\n",
    "            c_gradient = tape.gradient(c_loss, self.critic.trainable_variables)\n",
    "            self.c_optimizer.apply_gradients(\n",
    "                zip(c_gradient, self.critic.trainable_variables)\n",
    "            )\n",
    "\n",
    "        random_latent_vectors = tf.random.normal(\n",
    "            shape=(batch_size, self.latent_dim)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(\n",
    "                [random_latent_vectors, one_hot_labels], training=True\n",
    "            )\n",
    "            fake_predictions = self.critic(\n",
    "                [fake_images, image_one_hot_labels], training=True\n",
    "            )\n",
    "            g_loss = -tf.reduce_mean(fake_predictions)\n",
    "\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "\n",
    "        self.c_loss_metric.update_state(c_loss)\n",
    "        self.c_wass_loss_metric.update_state(c_wass_loss)\n",
    "        self.c_gp_metric.update_state(c_gp)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GAN\n",
    "cgan = ConditionalWGAN(\n",
    "    critic=critic,\n",
    "    generator=generator,\n",
    "    latent_dim=Z_DIM,\n",
    "    critic_steps=CRITIC_STEPS,\n",
    "    gp_weight=GP_WEIGHT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if LOAD_MODEL:\n",
    "#     cgan.load_weights(\"./checkpoint/checkpoint.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the GAN\n",
    "cgan.compile(\n",
    "    c_optimizer=optimizers.Adam(\n",
    "        learning_rate=LEARNING_RATE, beta_1=ADAM_BETA_1, beta_2=ADAM_BETA_2\n",
    "    ),\n",
    "    g_optimizer=optimizers.Adam(\n",
    "        learning_rate=LEARNING_RATE, beta_1=ADAM_BETA_1, beta_2=ADAM_BETA_2\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model save checkpoint\n",
    "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=\"./checkpoint/checkpoint.ckpt\",\n",
    "    save_weights_only=True,\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "tensorboard_callback = callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "\n",
    "class ImageGenerator(callbacks.Callback):\n",
    "    def __init__(self, num_img, latent_dim):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(\n",
    "            shape=(self.num_img, self.latent_dim)\n",
    "        )\n",
    "        # 0 label\n",
    "        zero_label = np.repeat([[1, 0]], self.num_img, axis=0)\n",
    "        generated_images = self.model.generator(\n",
    "            [random_latent_vectors, zero_label]\n",
    "        )\n",
    "        generated_images = generated_images * 127.5 + 127.5\n",
    "        generated_images = generated_images.numpy()\n",
    "        display(\n",
    "            generated_images,\n",
    "            save_to=\"./output/generated_img_%03d_label_0.png\" % (epoch),\n",
    "            cmap=None,\n",
    "        )\n",
    "\n",
    "        # 1 label\n",
    "        one_label = np.repeat([[0, 1]], self.num_img, axis=0)\n",
    "        generated_images = self.model.generator(\n",
    "            [random_latent_vectors, one_label]\n",
    "        )\n",
    "        generated_images = generated_images * 127.5 + 127.5\n",
    "        generated_images = generated_images.numpy()\n",
    "        display(\n",
    "            generated_images,\n",
    "            save_to=\"./output/generated_img_%03d_label_1.png\" % (epoch),\n",
    "            cmap=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final models\n",
    "generator.save(\"./models/generator\")\n",
    "critic.save(\"./models/critic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 label\n",
    "z_sample = np.random.normal(size=(10, Z_DIM))\n",
    "class_label = np.repeat([[1, 0]], 10, axis=0)\n",
    "imgs = cgan.generator.predict([z_sample, class_label])\n",
    "display(imgs, cmap=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 label\n",
    "z_sample = np.random.normal(size=(10, Z_DIM))\n",
    "class_label = np.repeat([[0, 1]], 10, axis=0)\n",
    "imgs = cgan.generator.predict([z_sample, class_label])\n",
    "display(imgs, cmap=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
