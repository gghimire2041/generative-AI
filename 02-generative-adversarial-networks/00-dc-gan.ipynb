{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Adversarial Networks (GANs) are a type of machine learning framework that pits two neural networks against each other, enabling the generation of new data that mimics an existing dataset.\n",
    "\n",
    "Here's how GANs work:\n",
    "\n",
    "1. **Generator:** One network, the generator, creates new data instances. It starts by generating random samples and aims to produce data that is indistinguishable from the real data it's been trained on. Initially, its generated samples might be poor, but through training, it improves its ability to create realistic data.\n",
    "\n",
    "2. **Discriminator:** The other network, the discriminator, evaluates the data it receives and tries to distinguish between real and generated data. Its task is to become proficient at differentiating the real data from the fake data generated by the generator.\n",
    "\n",
    "The training process involves a back-and-forth competition between these networks:\n",
    "\n",
    "- The generator tries to produce increasingly realistic data to fool the discriminator.\n",
    "- Simultaneously, the discriminator works to become better at identifying real from fake data.\n",
    "\n",
    "As training progresses, both networks improve. The generator gets better at creating realistic data, while the discriminator becomes more skilled at telling real from generated data.\n",
    "\n",
    "The ultimate aim is for the generator to produce data that's so convincing that the discriminator can't distinguish it from the real data.\n",
    "\n",
    "Applications of GANs span various fields, including image generation, video synthesis, text-to-image synthesis, and even generating realistic-sounding speech. They've been used in creating deep fakes, image enhancement, artistic style transfer, and more, showcasing their versatility in generating complex, high-dimensional data distributions.\n",
    "\n",
    "1. **Conditional GANs (cGANs):** These GANs condition both the generator and discriminator on some additional information, such as class labels, text descriptions, or other structured data. They allow for more control over the generated output.\n",
    "\n",
    "2. **Deep Convolutional GANs (DCGANs):** DCGANs leverage convolutional neural networks in both the generator and discriminator. They are specifically designed for image generation tasks, showing improved stability and performance.\n",
    "\n",
    "3. **Wasserstein GANs (WGANs):** WGANs use the Wasserstein distance (also known as Earth Moverâ€™s Distance) as a metric for training stability, leading to more stable training dynamics and better convergence properties.\n",
    "\n",
    "4. **Progressive GANs:** These networks gradually increase the resolution of generated images during training. They start with low-resolution images and incrementally add details, resulting in high-resolution, realistic outputs.\n",
    "\n",
    "5. **CycleGANs:** CycleGANs specialize in learning mappings between two different domains without paired data. They excel in image-to-image translation tasks, such as transforming images from summer to winter or changing the style of paintings without a one-to-one correspondence in the training data.\n",
    "\n",
    "6. **StyleGANs:** StyleGANs focus on controlling specific aspects of image generation, like image styles, attributes, or features. They have been used to generate highly realistic and diverse images with controllable visual features.\n",
    "\n",
    "7. **BigGANs:** These are GANs designed for generating high-quality images at high resolutions. They incorporate techniques like large batch training and architectures to generate high-fidelity images.\n",
    "\n",
    "8. **Self-Attention GANs (SAGANs):** SAGANs introduce self-attention mechanisms into GANs to enable better modeling of long-range dependencies in images, improving the generation quality and coherence.\n",
    "\n",
    "These types of GANs have distinct architectures or training methodologies tailored for specific challenges or enhancements in generating data, whether it's images, text, or other types of content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Deep Convolutional GANs (DCGANs):** DCGANs leverage convolutional neural networks in both the generator and discriminator. They are specifically designed for image generation tasks, showing improved stability and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Github Projects\\generative-AI\\gai\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import (\n",
    "    layers,\n",
    "    models,\n",
    "    callbacks,\n",
    "    losses,\n",
    "    utils,\n",
    "    metrics,\n",
    "    optimizers,\n",
    ")\n",
    "\n",
    "# from notebooks.utils import display, sample_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "CHANNELS = 1\n",
    "BATCH_SIZE = 128\n",
    "Z_DIM = 100\n",
    "EPOCHS = 300\n",
    "LOAD_MODEL = False\n",
    "ADAM_BETA_1 = 0.5\n",
    "ADAM_BETA_2 = 0.999\n",
    "LEARNING_RATE = 0.0002\n",
    "NOISE_PARAM = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = utils.image_dataset_from_directory(\n",
    "    \"dataset/\",\n",
    "    labels=None,\n",
    "    color_mode=\"grayscale\",\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    interpolation=\"bilinear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be Continued...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
